{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663e0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import scipy\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "#os.chdir('D:\\Computer Vision\\sparashar_p2\\Phase2')\n",
    "#os.chdir('C:\\Users\\gsaiu\\Downloads\\NeRF')\n",
    "# from PIL import Image\n",
    "\n",
    "class NerfDataset(Dataset):\n",
    "    def __init__(self, json_file, transform=None, mode='train'):\n",
    "        \n",
    "        #json_file = './transforms_' + mode + '.json'\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['frames'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_data = self.data['frames'][idx]\n",
    "        image_path = frame_data['file_path']\n",
    "        image_path = image_path + '.png'\n",
    "        transform_matrix = torch.tensor(frame_data['transform_matrix'], dtype=torch.float32)\n",
    "#         print(image_path)\n",
    "\n",
    "        # Load the image and apply transform (if provided)\n",
    "#         image = Image.open(image_path).convert('RGB')\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         print(image.shape)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Extract camera position and direction from the transformation matrix\n",
    "        cam_pos = transform_matrix[:3, 3]\n",
    "        cam_dir = -transform_matrix[:3, :3] @ torch.tensor([0, 0, 1], dtype=torch.float32)\n",
    "\n",
    "        # Get pixel coordinates of the image\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "#         print(type(h))\n",
    "        i, j = torch.meshgrid(torch.arange(h, dtype=torch.float32), torch.arange(w, dtype=torch.float32))\n",
    "        print(i, j)\n",
    "        pixel_coords = torch.stack([j, i, torch.ones_like(i)], dim=-1)\n",
    "#         print(transform_matrix.dtype)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Transform pixel coordinates into camera coordinates\n",
    "        ray_dir = pixel_coords @ torch.linalg.inv(transform_matrix[:3, :3]).T\n",
    "        ray_dir = ray_dir / torch.norm(ray_dir, dim=-1, keepdim=True)\n",
    "        ray_origins = cam_pos.repeat(h * w, 1)\n",
    "        \n",
    "\n",
    "        # Flatten the rays into a 2D tensor\n",
    "        rays = torch.cat([ray_origins, ray_dir.reshape(-1, 3)], dim=-1)\n",
    "\n",
    "        # Return the rays and image\n",
    "        return rays, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5416e408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NerfDataset at 0x256519efdc0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "json_file = './transforms_train.json'\n",
    "\n",
    "\n",
    "NerfDataset(json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf19e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.NerfDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0eaba2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('./train/r_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1dda9241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4cb62757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64000000, 3])\n"
     ]
    }
   ],
   "source": [
    "json_file = './transforms_train.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    json_obj = json.load(f)\n",
    "\n",
    "# extract the file paths from the JSON object\n",
    "file_paths = [frame['file_path'] for frame in json_obj['frames']]\n",
    "\n",
    "# read the images into a list\n",
    "images = []\n",
    "for file_path in file_paths:\n",
    "    image = Image.open(file_path + '.png').convert('RGB')\n",
    "    tensor = to_tensor(image)\n",
    "    # reshape the tensor to have shape (N, H*W)\n",
    "    tensor = tensor.view(3, -1)\n",
    "    # transpose the tensor to have shape (H*W, N)\n",
    "    tensor = tensor.transpose(0, 1)\n",
    "    images.append(tensor)\n",
    "\n",
    "# concatenate the tensors along the 0th dimension to create a tensor with shape (N*H*len(images), 3)\n",
    "images_tensor = torch.cat(images, dim=0)\n",
    "\n",
    "print(images_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "30a200f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focal_length(camera_angle_x, image_width=800):\n",
    "    return 0.5 * image_width / math.tan(camera_angle_x / 2)\n",
    "\n",
    "\n",
    "\n",
    "# def get_rays(H, W, focal, c2w):\n",
    "#     i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32), torch.arange(H, dtype=torch.float32))\n",
    "#     dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "#     rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "#     rays_o = torch.broadcast_to(torch.from_numpy(c2w[:3,-1]), rays_d.shape)\n",
    "    \n",
    "#     return rays_o, rays_d\n",
    "\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32), torch.arange(H, dtype=torch.float32))\n",
    "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = torch.broadcast_to(torch.from_numpy(c2w[:3,-1]), rays_d.shape)\n",
    "    rays_o = rays_o.view(-1, 3)\n",
    "    rays_d = rays_d.view(-1, 3)\n",
    "    \n",
    "    return rays_o, rays_d\n",
    "\n",
    "def get_rays_from_json(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    camera_angle_x = data['camera_angle_x']\n",
    "    focal_length = get_focal_length(camera_angle_x)\n",
    "    \n",
    "    rays_o_list = []\n",
    "    rays_d_list = []\n",
    "    \n",
    "    for frame in data['frames']:\n",
    "        transform_matrix = np.array(frame['transform_matrix'])\n",
    "        c2w = transform_matrix[:3, :4]\n",
    "        rays_o, rays_d = get_rays(800, 800, focal_length, c2w)\n",
    "    \n",
    "        rays_o_list.append(rays_o)\n",
    "        rays_d_list.append(rays_d)\n",
    "        \n",
    "    rays_o = torch.stack(rays_o_list, dim=0)\n",
    "    rays_d = torch.stack(rays_d_list, dim=0)\n",
    "    \n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bc64e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64000000, 3])\n",
      "torch.Size([64000000, 3])\n"
     ]
    }
   ],
   "source": [
    "ray_o,ray_d=get_rays_from_json(json_file)\n",
    "\n",
    "\n",
    "ray_o = ray_o.reshape(-1, 3)\n",
    "ray_d = ray_d.reshape(-1, 3)\n",
    "print(np.shape(ray_o))\n",
    "print(np.shape(ray_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "115bb232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "35c8d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train(nerf_model, optimizer, scheduler,device=device, hn, hf, nb_epochs,nb_bins, H, W,CheckPointPath,images_tensor_train,ray_o_train,ray_d_train,images_tensor_val,ray_o_val,ray_d_val,batch_size):\n",
    "#     training_loss = []\n",
    "#     validation_loss = []\n",
    "    \n",
    "#     for epoch_cur in tqdm(range(nb_epochs)):\n",
    "#         epoch_training_loss = []\n",
    "#         epoch_validation_loss = []\n",
    "#         for batch in ray_origins:\n",
    "#             ray_origins = batch[:, :3].to(device)\n",
    "#             ray_directions = batch[:, 3:6].to(device)\n",
    "#             ground_truth_px_values = batch[:, 6:].to(device)\n",
    "\n",
    "#             regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "#             train_loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             train_loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_training_loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "\n",
    "#         with torch.no_grad():\n",
    "#           for batch_idx in range(len(val_ray_origins)):\n",
    "#               ray_origins = val_ray_origins[batch_idx].to(device)\n",
    "#               ray_directions = val_ray_directions[batch_idx].to(device)\n",
    "#               ground_truth_px_values = val_ground_truth_px_values[batch_idx].to(device)\n",
    "\n",
    "#               regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "#               loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
    "#               epoch_validation_loss.append(loss.item())\n",
    "\n",
    "\n",
    "#         training_loss.append(sum(epoch_training_loss) / len(epoch_training_loss))\n",
    "#         validation_loss.append(sum(epoch_validation_loss) / len(epoch_validation_loss))           \n",
    "\n",
    "#         scheduler.step()\n",
    "#         SaveName = CheckPointPath + str(epoch_cur) + \"_model.ckpt\"\n",
    "#         torch.save(\n",
    "#             {\n",
    "#                 \"epoch\": epoch_cur,\n",
    "#                 \"model_state_dict\": model.state_dict(),\n",
    "#                 \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#                 \"loss\": train_loss,\n",
    "#             },\n",
    "#             SaveName,\n",
    "#         )\n",
    "#         print(\"\\n\" + SaveName + \" Model Saved...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     return training_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "38c5f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == 'main':\n",
    "    # device = 'cuda'\n",
    "    #training_dataset = torch.from_numpy(np.load('training_data.pkl', allow_pickle=True))\n",
    "    #testing_dataset = torch.from_numpy(np.load('testing_data.pkl', allow_pickle=True))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    model = NeRF(hidden_dim=256).to(device)\n",
    "    model_optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "    # with open(json_file, 'r') as f:\n",
    "    #     json_obj = json.load(f)\n",
    "\n",
    "    # # extract the file paths from the JSON object\n",
    "    # file_paths = [frame['file_path'] for frame in json_obj['frames']]\n",
    "\n",
    "    # # read the images into a list\n",
    "    # images = []\n",
    "    # for file_path in file_paths:\n",
    "    #     image = Image.open(file_path + '.png').convert('RGB')\n",
    "    #     images.append(image)\n",
    "\n",
    "    \n",
    "    # ray_o,ray_d=get_rays_from_json(json_file)\n",
    "\n",
    "\n",
    "    # ray_o = ray_o.reshape(-1, 3)\n",
    "    # ray_d = ray_d.reshape(-1, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    json_file_train = './transforms_train.json'\n",
    "\n",
    "    images_tensor_train,ray_o_train,ray_d_train=get_all_data(json_file_train)\n",
    "\n",
    "\n",
    "    json_file_val='./transforms_val.json'\n",
    "    images_tensor_val,ray_o_val,ray_d_val=get_all_data(json_file_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # with open(json_file, 'r') as f:\n",
    "    #     json_obj = json.load(f)\n",
    "\n",
    "    # # extract the file paths from the JSON object\n",
    "    # file_paths = [frame['file_path'] for frame in json_obj['frames']]\n",
    "\n",
    "    # # read the images into a list\n",
    "    # images = []\n",
    "    # for file_path in file_paths:\n",
    "    #     image = Image.open(file_path + '.png').convert('RGB')\n",
    "    #     tensor = to_tensor(image)\n",
    "    #     # reshape the tensor to have shape (N, H*W)\n",
    "    #     tensor = tensor.view(3, -1)\n",
    "    #     # transpose the tensor to have shape (H*W, N)\n",
    "    #     tensor = tensor.transpose(0, 1)\n",
    "    #     images.append(tensor)\n",
    "\n",
    "    # # concatenate the tensors along the 0th dimension to create a tensor with shape (N*H*len(images), 3)\n",
    "    # images_tensor = torch.cat(images, dim=0)\n",
    "\n",
    "    # #print(images_tensor.shape)\n",
    "    # ray_o,ray_d=get_rays_from_json(json_file)\n",
    "\n",
    "\n",
    "    # ray_o = ray_o.reshape(-1, 3)\n",
    "    # ray_d = ray_d.reshape(-1, 3)\n",
    "\n",
    "\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "    #data_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True)\n",
    "    \n",
    "    CheckPointPath='./Checkpoints/'\n",
    "\n",
    "    batch_size = 100\n",
    "\n",
    "    #train(model, model_optimizer, scheduler, device=device, hn=2, hf=6,  nb_epochs=100, nb_bins=192, H=800,W=800,CheckPointPath='./Checkpoints/',images_tensor_train=images_tensor_train,ray_o_train = ray_o_train,ray_d_train = ray_d_train,images_tensor_val = images_tensor_val,ray_o_val = ray_o_val,ray_d_val = ray_o_val,batch_size=batch_size)\n",
    "    train(images_tensor_train, ray_o_train, ray_d_train, images_tensor_val, ray_o_val, ray_d_val,model, model_optimizer, scheduler, hn=2, hf=6,nb_epochs=100, nb_bins=192, H=800, W=800,device=device, CheckPointPath=\"\", batch_size=100)\n",
    "    \n",
    "\n",
    "    # def train(images_tensor_train, ray_o_train, ray_d_train, images_tensor_val, ray_o_val, ray_d_val,\n",
    "    #       nerf_model, optimizer, scheduler, hn=2, hf=6, nb_epochs=100, nb_bins=192, H=800, W=800, \n",
    "    #       device=device, CheckPointPath=\"\", batch_size=100):\n",
    "    #train(nerf_model, optimizer, scheduler,device=device, hn, hf, nb_epochs,nb_bins, H, W,CheckPointPath,images_tensor_train,ray_o_train,ray_d_train,images_tensor_val,ray_o_val,ray_d_val):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6d348198",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('./train/r_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2b4e40cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3c29fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# define the input and output directories\n",
    "input_dir = './val'\n",
    "output_dir = './resized_val'\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# loop over all .png files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.png'):\n",
    "        # open the image\n",
    "        img = Image.open(os.path.join(input_dir, filename))\n",
    "\n",
    "        # resize the image to 100x100\n",
    "        img = img.resize((100, 100))\n",
    "\n",
    "        # save the resized image in the output directory\n",
    "        img.save(os.path.join(output_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9fc4b552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 3])\n",
      "torch.Size([1000000, 3])\n",
      "torch.Size([1000000, 3])\n",
      "torch.Size([1000000, 3])\n",
      "torch.Size([1000000, 3])\n",
      "torch.Size([1000000, 3])\n"
     ]
    }
   ],
   "source": [
    "json_file_train = './transforms_train.json'\n",
    "\n",
    "images_tensor_train,ray_o_train,ray_d_train=get_all_data(json_file_train)\n",
    "print(np.shape(images_tensor_train))\n",
    "print(np.shape(ray_o_train))\n",
    "print(np.shape(ray_d_train))\n",
    "\n",
    "\n",
    "\n",
    "json_file_val='./transforms_val.json'\n",
    "images_tensor_val,ray_o_val,ray_d_val=get_all_data(json_file_val)\n",
    "print(np.shape(images_tensor_val))\n",
    "print(np.shape(ray_o_val))\n",
    "print(np.shape(ray_d_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def getLoss(pred, labels):\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(pred, labels.view(-1, 8))\n",
    "    return loss\n",
    "\n",
    "class ModelBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = getLoss(out, labels)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = getLoss(out, labels)\n",
    "        return {'val_loss': loss.detach()}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeRF(ModelBase):\n",
    "\n",
    "    def __init__(self, embedding_dim_pos=3, embedding_dim_direction=3, hidden_dim=128):\n",
    "        super(NeRF, self).__init__()\n",
    "\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.embedding_dim_direction = embedding_dim_direction\n",
    "\n",
    "        self.block1 = self.make_block(embedding_dim_pos * 6 + 3, hidden_dim, hidden_dim)\n",
    "        self.block2 = self.make_block(hidden_dim + embedding_dim_pos * 6 + 3, hidden_dim, hidden_dim + 1)\n",
    "        self.block3 = self.make_block(hidden_dim + embedding_dim_direction * 6 + 3, hidden_dim // 2, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_block(in_dim, hidden_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_direction)\n",
    "\n",
    "        h = self.block1(emb_x)\n",
    "\n",
    "        h = torch.cat((h, emb_x), dim=1)\n",
    "        h = self.block2(h)\n",
    "        sigma = self.relu(h[:, -1])\n",
    "        h = h[:, :-1]\n",
    "\n",
    "        h = torch.cat((h, emb_d), dim=1)\n",
    "        h = self.block3(h)\n",
    "\n",
    "        c = torch.sigmoid(h)\n",
    "\n",
    "        return c, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "50e69603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from Network import NeRF\n",
    "import json\n",
    "import os\n",
    "from skimage import io\n",
    "import math\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import cv2\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_focal_length(camera_angle_x, image_width=100):\n",
    "    return 0.5 * image_width / math.tan(camera_angle_x / 2)\n",
    "\n",
    "\n",
    "\n",
    "# def get_rays(H, W, focal, c2w):\n",
    "#     i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32), torch.arange(H, dtype=torch.float32))\n",
    "#     dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "#     rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "#     rays_o = torch.broadcast_to(torch.from_numpy(c2w[:3,-1]), rays_d.shape)\n",
    "    \n",
    "#     return rays_o, rays_d\n",
    "\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32), torch.arange(H, dtype=torch.float32))\n",
    "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = torch.broadcast_to(torch.from_numpy(c2w[:3,-1]), rays_d.shape)\n",
    "    rays_o = rays_o.view(-1, 3)\n",
    "    rays_d = rays_d.view(-1, 3)\n",
    "    \n",
    "    return rays_o, rays_d\n",
    "\n",
    "def get_rays_from_json(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    camera_angle_x = data['camera_angle_x']\n",
    "    focal_length = get_focal_length(camera_angle_x)\n",
    "    \n",
    "    rays_o_list = []\n",
    "    rays_d_list = []\n",
    "    \n",
    "    for frame in data['frames']:\n",
    "        transform_matrix = np.array(frame['transform_matrix'])\n",
    "        c2w = transform_matrix[:3, :4]\n",
    "        rays_o, rays_d = get_rays(100, 100, focal_length, c2w)\n",
    "    \n",
    "        rays_o_list.append(rays_o)\n",
    "        rays_d_list.append(rays_d)\n",
    "        \n",
    "    rays_o = torch.stack(rays_o_list, dim=0)\n",
    "    rays_d = torch.stack(rays_d_list, dim=0)\n",
    "    \n",
    "    return rays_o, rays_d\n",
    "\n",
    "def image_to_tensor(image):\n",
    "    # Convert the image to a numpy array\n",
    "    np_array = np.array(image)\n",
    "    \n",
    "    plt.imshow(np_array)\n",
    "    plt.show()\n",
    "    \n",
    "    # (0,1,2)=(H,W,C)\n",
    "\n",
    "    # Transpose the array to have shape (C, H, W)\n",
    "    np_array = np.transpose(np_array, (2, 0, 1))\n",
    "\n",
    "    # Convert the numpy array to a torch tensor\n",
    "    tensor = torch.from_numpy(np_array).float()\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "\n",
    "    # create normalization transform\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    # apply normalization to tensor\n",
    "    normalized_tensor = normalize(tensor)\n",
    "    print(np.shape(normalized_tensor.permute(1, 2, 0)))\n",
    "    plt.imshow(normalized_tensor.permute(1, 2, 0), vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #tensor=tensor.view(-1,3)\n",
    "#     np_array = tensor.numpy()\n",
    "#     plt.imshow(np_array.transpose(1, 2, 0))\n",
    "#     plt.show()\n",
    "#     print(tensor)\n",
    "\n",
    "\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_data(json_file):\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_obj = json.load(f)\n",
    "\n",
    "    # extract the file paths from the JSON object\n",
    "    file_paths = [frame['file_path'] for frame in json_obj['frames']]\n",
    "\n",
    "    # read the images into a list\n",
    "    images = []\n",
    "#     for file_path in file_paths:\n",
    "# #         image = Image.open(file_path + '.png').convert('RGB')\n",
    "# #         print(image)\n",
    "\n",
    "#         img = cv2.imread(file_path + '.png')\n",
    "#         image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         #print(image[50][60])\n",
    "\n",
    "#         tensor = image_to_tensor(image)\n",
    "\n",
    "#         # reshape the tensor to have shape (N, H*W)\n",
    "#         tensor = tensor.view(3, -1)\n",
    "\n",
    "#         # transpose the tensor to have shape (H*W, N)\n",
    "#         tensor = tensor.transpose(0, 1)\n",
    "#         #print(tensor[1800])\n",
    "#         images.append(tensor)\n",
    "\n",
    "\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        image = Image.open(file_path + '.png').convert('RGB')\n",
    "#         print(np.shape(image))\n",
    "#         image1=np.array(image)\n",
    "#         plt.imshow(image1)\n",
    "#         plt.show()\n",
    "        \n",
    "       # print(np.shape(image))\n",
    "#         tensor = to_tensor(image)\n",
    "#         # reshape the tensor to have shape (N, H*W)\n",
    "#         tensor = tensor.view(3, -1)\n",
    "#         # transpose the tensor to have shape (H*W, N)\n",
    "#         tensor = tensor.transpose(0, 1)\n",
    "\n",
    "        tensor = to_tensor(image)\n",
    "        tensor = tensor.permute(1, 2, 0) \n",
    "        \n",
    "        \n",
    "        #tensor = tensor.transpose(0, 1).transpose(1, 2)\n",
    "        # convert the tensor to a numpy array\n",
    "        np_array = tensor.numpy()\n",
    "        # reshape the numpy array to the original shape of the image\n",
    "       # np_array = np_array.reshape(image.size[1], image.size[0], 3)\n",
    "        # plot the image\n",
    "        \n",
    "        \n",
    "#         print(np.shape(np_array))\n",
    "#         plt.imshow(np_array)\n",
    "#         plt.show()\n",
    "        \n",
    "        images.append(tensor)\n",
    "        \n",
    "\n",
    "\n",
    "# Convert the numpy array to a torch tensor\n",
    "    images_tensor = torch.cat(images, dim=0)\n",
    "    #print(images_tensor.shape)\n",
    "    ray_o,ray_d=get_rays_from_json(json_file)\n",
    "\n",
    "\n",
    "    ray_o = ray_o.reshape(-1, 3)\n",
    "    ray_d = ray_d.reshape(-1, 3)\n",
    "\n",
    "    return images_tensor, ray_o, ray_d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=2, hf=6, nb_bins=192):\n",
    "        # device = ray_origins.device\n",
    "\n",
    "        #evenly distributed points on the ray, with number of points nb_bins, the upper 'hf' and lower 'hn' bound for each ray\n",
    "        t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "\n",
    "\n",
    "         # Perturb sampling along each ray, splitting of t into lower and upper has been done to have same perturbation for each point.\n",
    "        mid = (t[:, :-1] + t[:, 1:]) / 2\n",
    "        lower = torch.cat((t[:, :1], mid), -1)\n",
    "        upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "        # u = torch.rand(t.shape, device=device)\n",
    "        t = lower + (upper - lower) * torch.rand(t.shape, device=device)  # [batch_size, nb_bins]\n",
    "        delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1) #differences between points, and one large number that is insignificant\n",
    "\n",
    "        x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)   # [batch_size, nb_bins, 3]\n",
    "        ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "        x=x.float()\n",
    "        ray_directions=ray_directions.float()\n",
    "\n",
    "        colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "        colors = colors.reshape(x.shape)\n",
    "        sigma = sigma.reshape(x.shape[:-1])\n",
    "        alpha = 1 - torch.exp(-sigma * delta)\n",
    "        accumulated_transmittance = torch.cumprod(1 - alpha, 1)\n",
    "        accumulated_transmittance = torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=device),accumulated_transmittance[:, :-1]), dim=-1)\n",
    "        weights = accumulated_transmittance.unsqueeze(2) * alpha.unsqueeze(2)\n",
    "        c = (weights * colors).sum(dim=1)  # Pixel values\n",
    "        weight_sum = weights.sum(-1).sum(-1)  # Regularization for white background\n",
    "        return c + 1 - weight_sum.unsqueeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(images_tensor_train, ray_o_train, ray_d_train, images_tensor_val, ray_o_val, ray_d_val,\n",
    "          nerf_model, optimizer, scheduler, hn=2, hf=6, nb_epochs=100, nb_bins=192, H=100, W=100, \n",
    "          device=device, CheckPointPath=\"\", batch_size=100):\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    last_100_batches_px_values = []\n",
    "    \n",
    "\n",
    "    n_batches = len(ray_o_train) // batch_size\n",
    "    print(n_batches)\n",
    "    \n",
    "    train_batches = [(ray_o_train[i*batch_size:(i+1)*batch_size], ray_d_train[i*batch_size:(i+1)*batch_size], images_tensor_train[i*batch_size:(i+1)*batch_size]) for i in range(n_batches)]\n",
    "    \n",
    "    for epoch_cur in tqdm(range(nb_epochs)):\n",
    "        epoch_training_loss = []\n",
    "        last_100_batches=[]\n",
    "        epoch_validation_loss = []\n",
    "        batch_num=0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for batch in train_batches:\n",
    "            ray_origins = batch[0].to(device)\n",
    "            ray_directions = batch[1].to(device)\n",
    "            ground_truth_px_values = batch[2].to(device)\n",
    "\n",
    "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "            #print(np.shape(regenerated_px_values))\n",
    "            train_loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_training_loss.append(train_loss.item())\n",
    "            # if batch_num >= n_batches - 100:  # Only append last 100 batches of epoch\n",
    "            #     last_100_batches.append(regenerated_px_values.detach().cpu().numpy())\n",
    "            # batch_num+=1\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        # last_100_batches_arr = np.concatenate(last_100_batches, axis=0)\n",
    "\n",
    "        # # Reshape to a 100x100x3 numpy array\n",
    "        # last_100_batches_arr = last_100_batches_arr.reshape((100, 100, 3))\n",
    "\n",
    "        # # Save as an image using OpenCV\n",
    "        # cv2.imwrite(f\"./Images/epoch_{epoch_cur}.png\", last_100_batches_arr)\n",
    "        # # Clear the last_100_batches list for the next epoch\n",
    "        # last_100_batches.clear()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "           #print(train_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        training_loss.append(sum(epoch_training_loss) / len(epoch_training_loss))\n",
    "        print(\"epoch loss\")\n",
    "        print(sum(epoch_training_loss) / len(epoch_training_loss))\n",
    "        #validation_loss.append(sum(epoch_validation_loss) / len(epoch_validation_loss))\n",
    "        #print(sum(epoch_validation_loss) / len(epoch_validation_loss))\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        \n",
    "        SaveName = CheckPointPath + str(epoch_cur) + \"_model.ckpt\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch_cur,\n",
    "                \"model_state_dict\": nerf_model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": train_loss,\n",
    "            },\n",
    "            SaveName,\n",
    "        )\n",
    "        print(\"\\n\" + SaveName + \" Model Saved...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for i in range(100):\n",
    "        #         val_ray_origins = ray_o_val[i*100*100:(i+1)*100*100]\n",
    "        #         val_ray_directions = ray_d_val[i*100*100:(i+1)*100*100]\n",
    "        #         #val_ground_truth_px_values = images_tensor_val.to(device)\n",
    "        #         image=[]\n",
    "        #         for i in range(0,H*W,batch_size):\n",
    "        #             val_ray_origins=val_ray_origins[i:i+batch_size].to(device)\n",
    "        #             val_ray_directions = val_ray_directions[i:i+batch_size].to(device)\n",
    "        #             test_px = render_rays(nerf_model, val_ray_origins, val_ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "        #             image.append(test_px)\n",
    "        #         test_image=torch.cat(image,dim=0)\n",
    "        #         test_image=test_image.cpu().numpy().reshape((100,100,3))\n",
    "        #         cv2.imwrite(f\"./Images/{i}.png\", test_image)\n",
    "        with torch.no_grad():\n",
    "            for i in range(100):\n",
    "                val_ray_origins = ray_o_val[i*100*100:(i+1)*100*100]\n",
    "                val_ray_directions = ray_d_val[i*100*100:(i+1)*100*100]\n",
    "                image=[]\n",
    "                for j in range(0, H*W, batch_size):\n",
    "                    val_ray_origins_batch = val_ray_origins[j:j+batch_size].to(device)\n",
    "                    val_ray_directions_batch = val_ray_directions[j:j+batch_size].to(device)\n",
    "                    test_px = render_rays(nerf_model, val_ray_origins_batch, val_ray_directions_batch, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "                    image.append(test_px)\n",
    "                test_image=torch.cat(image, dim=0)\n",
    "                test_image=test_image.cpu().numpy().reshape((H, W, 3))\n",
    "                cv2.imwrite(f\"./Images/{i}.png\", test_image)    \n",
    "\n",
    "    return training_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "196d0e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0538,  3.8455,  1.2081], dtype=torch.float64)\n",
      "tensor([ 0.2452, -1.0586,  0.0438], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "json_file_train='./transforms_train.json'\n",
    "images_tensor_train,ray_o_train,ray_d_train=get_all_data(json_file_train)\n",
    "#print(images_tensor_train[20000])\n",
    "\n",
    "print((images_tensor_train[50][20][2]))\n",
    "plt.imshow(images_tensor_train[:,90,:].reshape(100,100,3))\n",
    "plt.show()\n",
    "print(ray_o_train[1000])\n",
    "print(ray_d_train[1800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7dc84be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(json_file):\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_obj = json.load(f)\n",
    "\n",
    "\n",
    "    file_paths = [frame['file_path'] for frame in json_obj['frames']]\n",
    "\n",
    "    # read the images into a list\n",
    "    images = []\n",
    "    concatenated_data = None\n",
    "\n",
    "\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        image = Image.open(file_path + '.png').convert('RGB')\n",
    "        #print(np.shape(image))\n",
    "#         print(np.shape(image))\n",
    "#         image1=np.array(image)\n",
    "#         plt.imshow(image1)\n",
    "#         plt.show()\n",
    "        \n",
    "       # print(np.shape(image))\n",
    "#         tensor = to_tensor(image)\n",
    "#         # reshape the tensor to have shape (N, H*W)\n",
    "#         tensor = tensor.view(3, -1)\n",
    "#         # transpose the tensor to have shape (H*W, N)\n",
    "#         tensor = tensor.transpose(0, 1)\n",
    "\n",
    "        tensor = to_tensor(image)\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "#         print(np.shape(tensor))\n",
    "#         plt.imshow(tensor)\n",
    "#         plt.show()\n",
    "        \n",
    "        tensor=tensor.view(-1,3)\n",
    "        #print(np.shape(tensor))\n",
    "        \n",
    "#         tensor=tensor.view(100,100,3)     \n",
    "#         print(np.shape(tensor))\n",
    "#         plt.imshow(tensor)\n",
    "#         plt.show()\n",
    "\n",
    "        if concatenated_data is None:\n",
    "            concatenated_data = tensor\n",
    "        else:\n",
    "            concatenated_data = torch.cat((concatenated_data, tensor), dim=0)\n",
    "\n",
    "        #tensor = tensor.transpose(0, 1).transpose(1, 2)\n",
    "        # convert the tensor to a numpy array\n",
    "        np_array = tensor.numpy()\n",
    "        # reshape the numpy array to the original shape of the image\n",
    "       # np_array = np_array.reshape(image.size[1], image.size[0], 3)\n",
    "        # plot the image\n",
    "        \n",
    "        \n",
    "#         print(np.shape(np_array))\n",
    "#         plt.imshow(np_array)\n",
    "#         plt.show()\n",
    "        \n",
    "        images.append(tensor)\n",
    "        \n",
    "\n",
    "    #print(type(concatenated_data))\n",
    "# Convert the numpy array to a torch tensor\n",
    "    images_tensor = torch.cat(images, dim=0)\n",
    "    #print(images_tensor.shape)\n",
    "    ray_o,ray_d=get_rays_from_json(json_file)\n",
    "\n",
    "\n",
    "    ray_o = ray_o.reshape(-1, 3)\n",
    "    ray_d = ray_d.reshape(-1, 3)\n",
    "\n",
    "    return concatenated_data, ray_o, ray_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "496ad7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 3])\n",
      "torch.Size([100, 100, 100, 3])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for dimension 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10548\\840573991.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_tensor_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# print((images_tensor_train[50][20][2]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_tensor_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 100 is out of bounds for dimension 0 with size 100"
     ]
    }
   ],
   "source": [
    "json_file_train='./transforms_train.json'\n",
    "images_tensor_train,ray_o_train,ray_d_train=get_all_data(json_file_train)\n",
    "print(images_tensor_train.shape)\n",
    "images_tensor_train=images_tensor_train.view(100,100,100,3)\n",
    "print(images_tensor_train.shape)\n",
    "# print((images_tensor_train[50][20][2]))\n",
    "plt.imshow(images_tensor_train[99].numpy())\n",
    "plt.show()\n",
    "\n",
    "# print(ray_o_train[1000])\n",
    "# print(ray_d_train[1800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "118336ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue channel min value: 0.0\n",
      "Blue channel max value: 255.0\n",
      "Green channel min value: 0.0\n",
      "Green channel max value: 255.0\n",
      "Red channel min value: 0.0\n",
      "Red channel max value: 255.0\n",
      "Blue channel mean value: 14.5511\n",
      "Green channel mean value: 22.3482\n",
      "Red channel mean value: 25.7996\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e26b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
